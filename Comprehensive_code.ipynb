{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**⚠️ DISCLAIMER** : *Uses open-source price data via yfinance. Not recommended for immediate commercial use. Deploy only after using verified brokerage data and thorough backtesting.*"
      ],
      "metadata": {
        "id": "CWFrsGQw3qKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTEBOOK OVERVIEW And Sequential Walkthrough-\n",
        "1. Installations\n",
        "2. Imports\n",
        "3. Data Loading And Cleaning\n",
        "4. Feature Engineering-\n",
        "\n",
        "  (Based on Research Paper-\n",
        "\n",
        "  \"Patterns in High Frequency Data: Discovery of 12 Empirical Laws\"\n",
        "\n",
        "  by J.B. Glattfelder, A. Dupuis, R.B. Olsen)\n",
        "\n",
        "5. Target Defining (based on a state space model- Kalman Filter)\n",
        "6. Target balancing, hyperparameter optimization, feature selection, etc\n",
        "7. Model training, testing, evaluation (Used XGboost Classifier)\n",
        "8. Live Prediction\n",
        "9. Getiing a Buy list of stocks\n",
        "10. Recommendations\n",
        "11. Kalman Filter Fair Price Metrics, Plot\n",
        "12. Backtest of Strategy (With a Tearsheet + VOLATILITY TARGETTING)"
      ],
      "metadata": {
        "id": "u-cvkYKNTFNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*INSTALLATIONS*"
      ],
      "metadata": {
        "id": "5pufx9OmGS2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install ta\n",
        "!pip install pykalman\n",
        "!pip install quantstats\n",
        "!pip install ipywidgets\n",
        "!pip install IPython"
      ],
      "metadata": {
        "id": "sVZJ9njsZSEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b99441-44f0-4fa9-b51c-726079bbe0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ta) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ta) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=5562c494379b453c652a6c1bf55d9cf8907abcf02e1339fd7ea08ed7be207fa1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n",
            "Collecting pykalman\n",
            "  Downloading pykalman-0.10.1-py2.py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from pykalman) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pykalman) (24.2)\n",
            "Collecting scikit-base<0.13.0 (from pykalman)\n",
            "  Downloading scikit_base-0.12.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: scipy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from pykalman) (1.15.3)\n",
            "Downloading pykalman-0.10.1-py2.py3-none-any.whl (248 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.5/248.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_base-0.12.3-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.5/145.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-base, pykalman\n",
            "Successfully installed pykalman-0.10.1 scikit-base-0.12.3\n",
            "Collecting quantstats\n",
            "  Downloading QuantStats-0.0.64-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from quantstats) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from quantstats) (2.0.2)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from quantstats) (0.13.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from quantstats) (3.10.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from quantstats) (1.15.3)\n",
            "Requirement already satisfied: tabulate>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from quantstats) (0.9.0)\n",
            "Requirement already satisfied: yfinance>=0.1.70 in /usr/local/lib/python3.11/dist-packages (from quantstats) (0.2.62)\n",
            "Requirement already satisfied: python-dateutil>=2.0 in /usr/local/lib/python3.11/dist-packages (from quantstats) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->quantstats) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->quantstats) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->quantstats) (4.58.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->quantstats) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->quantstats) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->quantstats) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->quantstats) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->quantstats) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.0->quantstats) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.0->quantstats) (1.17.0)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance>=0.1.70->quantstats) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance>=0.1.70->quantstats) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance>=0.1.70->quantstats) (4.3.8)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance>=0.1.70->quantstats) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance>=0.1.70->quantstats) (3.18.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance>=0.1.70->quantstats) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance>=0.1.70->quantstats) (0.11.3)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from yfinance>=0.1.70->quantstats) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.11/dist-packages (from yfinance>=0.1.70->quantstats) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance>=0.1.70->quantstats) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance>=0.1.70->quantstats) (4.14.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance>=0.1.70->quantstats) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance>=0.1.70->quantstats) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance>=0.1.70->quantstats) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance>=0.1.70->quantstats) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance>=0.1.70->quantstats) (2.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance>=0.1.70->quantstats) (2.22)\n",
            "Downloading QuantStats-0.0.64-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: quantstats\n",
            "Successfully installed quantstats-0.0.64\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.25.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from IPython) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTS"
      ],
      "metadata": {
        "id": "J-7qIb35GZRJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uqcvECGYcwf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "b6c5e370-0241-4ca6-9085-c28acb344e5e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'optuna'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1426855023>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeSeriesSplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moptuna\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_study\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTPESampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# CELL 1: Import Libraries and Basic Setup\n",
        "\n",
        "import yfinance as yf\n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import pandas as pd\n",
        "import ta\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from optuna import create_study\n",
        "from optuna.samplers import TPESampler\n",
        "import joblib\n",
        "from xgboost import DMatrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from scipy.stats import skew, kurtosis\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
        "import joblib\n",
        "import os\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from statsmodels.regression.linear_model import OLS\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from tqdm import tqdm\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from datetime import date\n",
        "from pprint import pprint\n",
        "import json\n",
        "from numpy import log, polyfit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.inspection import permutation_importance\n",
        "import shap\n",
        "import warnings\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import quantstats as qs\n",
        "import numpy as np\n",
        "from pykalman import KalmanFilter\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning, module='statsmodels\\\\.regression\\\\.linear_model')\n",
        "warnings.filterwarnings('ignore', category=UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA LOADING , CLEANING"
      ],
      "metadata": {
        "id": "5YnX_0PLGdVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Data Loading and Cleaning\n",
        "\n",
        "def get_nifty50_tickers():\n",
        "    \"\"\"\n",
        "    Modified to use only POWERGRID.NS for testing\n",
        "    In production, uncomment the full list\n",
        "    \"\"\"\n",
        "    # return [\"TATACONSUM.NS\"]\n",
        "\n",
        "    # # Full NIFTY 50 list (commented for testing)\n",
        "    # return [\n",
        "    #     \"ADANIENT.NS\", \"ADANIPORTS.NS\", \"ADANIGREEN.NS\",\n",
        "    #     \"ASIANPAINT.NS\", \"AXISBANK.NS\", \"BAJFINANCE.NS\", \"BAJAJFINSV.NS\",\n",
        "    #     \"BPCL.NS\", \"BRITANNIA.NS\", \"CIPLA.NS\", \"COALINDIA.NS\", \"DIVISLAB.NS\",\n",
        "    #     \"DRREDDY.NS\", \"EICHERMOT.NS\", \"GAIL.NS\", \"GRASIM.NS\", \"HCLTECH.NS\",\n",
        "    #     \"HDFCBANK.NS\", \"HEROMOTOCO.NS\", \"HINDUNILVR.NS\",\n",
        "    #     \"ICICIBANK.NS\", \"INDUSINDBK.NS\", \"INFY.NS\", \"ITC.NS\", \"JSWSTEEL.NS\",\n",
        "    #     \"KOTAKBANK.NS\", \"LT.NS\", \"M&M.NS\", \"MARUTI.NS\", \"NESTLEIND.NS\",\n",
        "    #     \"NTPC.NS\", \"ONGC.NS\", \"POWERGRID.NS\", \"RELIANCE.NS\", \"SBIN.NS\",\n",
        "    #     \"SHREECEM.NS\", \"SUNPHARMA.NS\", \"TATACONSUM.NS\", \"TATAMOTORS.NS\",\n",
        "    #     \"TATASTEEL.NS\", \"TECHM.NS\", \"TITAN.NS\", \"ULTRACEMCO.NS\", \"WIPRO.NS\",\n",
        "    #     \"UPL.NS\", \"IOC.NS\", \"TCS.NS\"\n",
        "    # ]\n",
        "\n",
        "    return [\n",
        "        \"ASIANPAINT.NS\", \"COALINDIA.NS\", \"DIVISLAB.NS\",\n",
        "        \"DRREDDY.NS\", \"EICHERMOT.NS\", \"GAIL.NS\", \"GRASIM.NS\", \"M&M.NS\", \"MARUTI.NS\",\n",
        "        \"NESTLEIND.NS\", \"POWERGRID.NS\", \"RELIANCE.NS\", \"TATACONSUM.NS\", \"TATAMOTORS.NS\",\n",
        "        \"TITAN.NS\", \"ULTRACEMCO.NS\", \"TCS.NS\"\n",
        "    ]\n",
        "\n",
        "# Load historical data\n",
        "stock_data_dict = {}\n",
        "end_date = date.today() - relativedelta(years=1)\n",
        "start_date = end_date - relativedelta(years=20)\n",
        "\n",
        "for ticker in get_nifty50_tickers():\n",
        "    try:\n",
        "        data = yf.download(ticker, start=start_date, end=end_date, auto_adjust=False, progress=False)\n",
        "        data = data.rename(columns={\n",
        "            'Open': 'open',\n",
        "            'High': 'high',\n",
        "            'Low': 'low',\n",
        "            'Close': 'close',\n",
        "            'Adj Close': 'adj_close',\n",
        "            'Volume': 'volume'\n",
        "        }).reset_index()\n",
        "        data['turnover'] = data['volume'] * data['close']\n",
        "        data = data[['Date', 'open', 'high', 'low', 'close', 'volume', 'turnover']]\n",
        "        data = data.rename(columns={'Date': 'timestamp'})\n",
        "        stock_data_dict[ticker] = data.dropna()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process {ticker}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "def clean_stock_data(stock_dict):\n",
        "    \"\"\"Clean and prepare stock data\"\"\"\n",
        "    cleaned_dict = {}\n",
        "    for ticker, df in stock_dict.items():\n",
        "        try:\n",
        "            if isinstance(df.columns, pd.MultiIndex):\n",
        "                df.columns = df.columns.droplevel(1)\n",
        "            df = df.copy()\n",
        "            df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover']]\n",
        "            df.set_index('timestamp', inplace=True)\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            df.sort_index(inplace=True)\n",
        "            df = df[~df.index.duplicated()]\n",
        "            df.dropna(inplace=True)\n",
        "            cleaned_dict[ticker] = df\n",
        "        except Exception as e:\n",
        "            print(f\"Cleaning failed for {ticker}: {str(e)}\")\n",
        "            continue\n",
        "    return cleaned_dict\n",
        "\n",
        "cleaned_data = clean_stock_data(stock_data_dict)\n",
        "print(f\"Successfully loaded and cleaned data for {len(cleaned_data)} stocks\")"
      ],
      "metadata": {
        "id": "b6NS2xypZL8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "S2635sH5Gmjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Feature Engineering Functions (Reduced Complexity for Overfitting Control)\n",
        "\n",
        "# ---- LAW 0a: Volatility Scaling (Absolute Return) Features ----\n",
        "def roll_abs_return_mean(arr):\n",
        "    ret = np.diff(arr) / arr[:-1]\n",
        "    return np.mean(np.abs(ret))\n",
        "\n",
        "def roll_abs_return_rms(arr):\n",
        "    ret = np.diff(arr) / arr[:-1]\n",
        "    return np.sqrt(np.mean(ret**2))\n",
        "\n",
        "def roll_abs_return_var(arr):\n",
        "    ret = np.diff(arr) / arr[:-1]\n",
        "    return np.var(ret)\n",
        "\n",
        "def roll_abs_return_median(arr):\n",
        "    ret = np.diff(arr) / arr[:-1]\n",
        "    return np.median(np.abs(ret))\n",
        "\n",
        "# Advanced Volatility Features (Law 0a Variants)\n",
        "def calc_roll_median_abs_return(arr):\n",
        "    ret = np.diff(arr) / arr[:-1]\n",
        "    return np.median(np.abs(ret))\n",
        "\n",
        "def calc_volatility_ratio(df, short_window=5, long_window=20):\n",
        "    vol_short = df['close'].pct_change().abs().rolling(window=short_window, min_periods=short_window).mean()\n",
        "    vol_long = df['close'].pct_change().abs().rolling(window=long_window, min_periods=long_window).mean()\n",
        "    return vol_short / vol_long\n",
        "\n",
        "# ---- LAW 0b: Directional Change Count Features ----\n",
        "def directional_change_count(arr, threshold):\n",
        "    count = 0\n",
        "    direction = None\n",
        "    last_extreme = arr[0]\n",
        "    for price in arr:\n",
        "        change = (price - last_extreme) / last_extreme\n",
        "        if direction is None:\n",
        "            direction = 'up' if change >= 0 else 'down'\n",
        "        else:\n",
        "            if direction == 'up' and change <= -threshold:\n",
        "                count += 1\n",
        "                direction = 'down'\n",
        "                last_extreme = price\n",
        "            elif direction == 'down' and change >= threshold:\n",
        "                count += 1\n",
        "                direction = 'up'\n",
        "                last_extreme = price\n",
        "    return count\n",
        "\n",
        "# ---- LAW 1: Tick-Count Scaling Law Approximation ----\n",
        "def tick_count_approx(high, low, close, tick_threshold=0.0002):\n",
        "    return (high - low) / (tick_threshold * close)\n",
        "\n",
        "# ---- LAW 2: Frequency of Price Moves Features ----\n",
        "def roll_move_count(arr, threshold):\n",
        "    ret = np.diff(arr) / arr[:-1]\n",
        "    return np.sum(np.abs(ret) >= threshold)\n",
        "\n",
        "# ---- LAW 3: Maximum Price Range Features ----\n",
        "def roll_max_range(high_arr, low_arr):\n",
        "    return np.max(high_arr) - np.min(low_arr)\n",
        "\n",
        "def roll_range_std(high_arr, low_arr):\n",
        "    ranges = high_arr - low_arr\n",
        "    return np.std(ranges)\n",
        "\n",
        "# ---- LAW 4: Time Between Moves Features ----\n",
        "def avg_time_between_moves(arr, threshold):\n",
        "    ret = np.abs(np.diff(arr) / arr[:-1])\n",
        "    idx = np.where(ret >= threshold)[0]\n",
        "    if len(idx) < 2:\n",
        "        return np.nan\n",
        "    return np.mean(np.diff(idx))\n",
        "\n",
        "# ---- KALMAN FILTER FAIR PRICE CALCULATION ----\n",
        "def calculate_kalman_fair_price(df, process_noise=1e-4, observation_noise=1e-2):\n",
        "    \"\"\"\n",
        "    Calculate Kalman filter fair price for the entire time series\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame with 'close' column\n",
        "    - process_noise: Process noise parameter (smaller = more stable)\n",
        "    - observation_noise: Observation noise parameter (smaller = more responsive)\n",
        "\n",
        "    Returns:\n",
        "    - Series of fair prices\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use log prices for better stability\n",
        "        log_prices = np.log(df['close'].values).reshape(-1, 1)\n",
        "\n",
        "        # Initialize Kalman Filter\n",
        "        kf = KalmanFilter(\n",
        "            initial_state_mean=log_prices[0],\n",
        "            n_dim_obs=1,\n",
        "            n_dim_state=1,\n",
        "            transition_matrices=np.array([[1.0]]),\n",
        "            observation_matrices=np.array([[1.0]]),\n",
        "            transition_covariance=np.array([[process_noise]]),\n",
        "            observation_covariance=np.array([[observation_noise]])\n",
        "        )\n",
        "\n",
        "        # Fit the filter\n",
        "        state_means, _ = kf.filter(log_prices)\n",
        "\n",
        "        # Convert back to price space\n",
        "        fair_prices = np.exp(state_means.flatten())\n",
        "\n",
        "        return pd.Series(fair_prices, index=df.index)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Kalman filter calculation failed: {e}\")\n",
        "        # Fallback to simple moving average\n",
        "        return df['close'].rolling(window=20, min_periods=1).mean()\n",
        "\n",
        "print(\"Feature engineering functions defined successfully!\")"
      ],
      "metadata": {
        "id": "Uz7IFl4nZ8JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Simplified Feature Engineering Implementation (Overfitting Reduction)\n",
        "\n",
        "def feature_engineering_simplified(df):\n",
        "    \"\"\"\n",
        "    Compute simplified features based on scaling laws to reduce overfitting.\n",
        "    This version uses fewer windows and features compared to the original.\n",
        "    \"\"\"\n",
        "    original_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
        "    df = df.copy()\n",
        "    df.sort_values('timestamp', inplace=True)\n",
        "\n",
        "    feature_dict = {}\n",
        "\n",
        "    # REDUCED WINDOWS FOR OVERFITTING CONTROL\n",
        "    # Original had many windows, this version uses only essential ones\n",
        "    windows_0a = [10, 20, 60]  # Reduced from [5, 10, 20, 30, 60, 120]\n",
        "    windows_0b = [20, 60]      # Reduced from [20, 30, 60]\n",
        "    windows_1 = [10, 20]       # Reduced from [5, 10, 20, 30, 60]\n",
        "    windows_2 = [20, 60]       # Reduced from [20, 30, 60]\n",
        "    windows_3 = [20, 60]       # Reduced from [10, 20, 30, 60, 120]\n",
        "    windows_4 = [20, 60]       # Reduced from [20, 30, 60]\n",
        "\n",
        "    # REDUCED THRESHOLDS\n",
        "    thresholds_0b = [0.02, 0.05]    # Reduced from [0.01, 0.02, 0.03, 0.05]\n",
        "    thresholds_2 = [0.05, 0.10]     # Reduced from [0.03, 0.05, 0.10, 0.15, 0.20]\n",
        "    thresholds_4 = [0.05, 0.10]     # Reduced from [0.03, 0.05, 0.10, 0.15]\n",
        "\n",
        "    # ----- LAW 0a: Volatility Scaling Features (Simplified) -----\n",
        "    for w in windows_0a:\n",
        "        feature_dict[f\"roll_abs_return_mean_{w}\"] = df['close'].rolling(window=w, min_periods=w).apply(lambda x: roll_abs_return_mean(x.values))\n",
        "        feature_dict[f\"roll_abs_return_rms_{w}\"] = df['close'].rolling(window=w, min_periods=w).apply(lambda x: roll_abs_return_rms(x.values))\n",
        "\n",
        "    # Only one volatility ratio instead of multiple\n",
        "    feature_dict[\"volatility_ratio_5_20\"] = calc_volatility_ratio(df, short_window=5, long_window=20)\n",
        "\n",
        "    # ----- LAW 0b: Directional Change Count Features (Simplified) -----\n",
        "    for w in windows_0b:\n",
        "        for thr in thresholds_0b:\n",
        "            fname = f\"roll_dir_change_{w}_thr_{int(thr*100)}\"\n",
        "            feature_dict[fname] = df['close'].rolling(window=w, min_periods=w).apply(lambda x: directional_change_count(x.values, thr))\n",
        "\n",
        "    # ----- LAW 1: Tick-Count Approximation (Simplified) -----\n",
        "    tick_threshold = 0.0002\n",
        "    df[\"temp_tick_count\"] = (df[\"high\"] - df[\"low\"]) / (tick_threshold * df[\"close\"])\n",
        "    for w in windows_1:\n",
        "        fname = f\"roll_tick_count_{w}\"\n",
        "        feature_dict[fname] = df[\"temp_tick_count\"].rolling(window=w, min_periods=w).mean()\n",
        "    df.drop(columns=[\"temp_tick_count\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    # ----- LAW 2: Frequency of Price Moves Features (Simplified) -----\n",
        "    for w in windows_2:\n",
        "        for thr in thresholds_2:\n",
        "            fname = f\"roll_move_count_{w}_thr_{int(thr*100)}\"\n",
        "            feature_dict[fname] = df['close'].pct_change().rolling(window=w, min_periods=w).apply(lambda x: np.sum(np.abs(x.values[1:]) >= thr))\n",
        "\n",
        "    # ----- LAW 3: Maximum Price Range Features (Simplified) -----\n",
        "    for w in windows_3:\n",
        "        range_series = df['high'] - df['low']\n",
        "        fname = f\"roll_max_range_{w}\"\n",
        "        feature_dict[fname] = df['high'].rolling(window=w, min_periods=w).max() - df['low'].rolling(window=w, min_periods=w).min()\n",
        "        fname_std = f\"roll_range_std_{w}\"\n",
        "        feature_dict[fname_std] = range_series.rolling(window=w, min_periods=w).std()\n",
        "\n",
        "    # ----- LAW 4: Time Between Moves Features (Simplified) -----\n",
        "    for w in windows_4:\n",
        "        for thr in thresholds_4:\n",
        "            fname = f\"roll_avg_time_between_{w}_thr_{int(thr*100)}\"\n",
        "            feature_dict[fname] = df['close'].rolling(window=w, min_periods=w).apply(lambda x: avg_time_between_moves(x.values, thr))\n",
        "\n",
        "    # ----- KALMAN FAIR PRICE FEATURE -----\n",
        "    df['kalman_fair_price'] = calculate_kalman_fair_price(df)\n",
        "\n",
        "    # Price vs Fair Price ratios\n",
        "    df['price_to_fair_ratio'] = df['close'] / df['kalman_fair_price']\n",
        "    df['fair_price_deviation'] = (df['close'] - df['kalman_fair_price']) / df['kalman_fair_price']\n",
        "\n",
        "    # Add these as features\n",
        "    feature_dict['price_to_fair_ratio'] = df['price_to_fair_ratio']\n",
        "    feature_dict['fair_price_deviation'] = df['fair_price_deviation']\n",
        "\n",
        "    # Create features DataFrame\n",
        "    features_df = pd.DataFrame(feature_dict)\n",
        "    df = pd.concat([df, features_df], axis=1)\n",
        "\n",
        "    # Shift features to avoid lookahead bias\n",
        "    all_feature_names = list(feature_dict.keys())\n",
        "    df[all_feature_names] = df[all_feature_names].shift(1)\n",
        "\n",
        "    # Handle NaN values\n",
        "    df.dropna(how='all', inplace=True)\n",
        "    df.ffill(inplace=True)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Create feature mapping\n",
        "    extended_original_cols = original_cols + ['kalman_fair_price', 'price_to_fair_ratio', 'fair_price_deviation']\n",
        "    sorted_feature_names = extended_original_cols + sorted(all_feature_names)\n",
        "    feature_mapping = {col: f\"{i+1:03d}\" for i, col in enumerate(sorted_feature_names)}\n",
        "    df.rename(columns=feature_mapping, inplace=True)\n",
        "\n",
        "\n",
        "    return df, feature_mapping\n",
        "\n",
        "def apply_feature_engineering_simplified(data_dict):\n",
        "    \"\"\"Apply simplified feature engineering to all stocks\"\"\"\n",
        "    fe_data = {}\n",
        "    feature_mappings = {}\n",
        "\n",
        "    for ticker, df in data_dict.items():\n",
        "        print(f\"Processing features for {ticker}...\")\n",
        "        engineered_df, mapping = feature_engineering_simplified(df)\n",
        "        fe_data[ticker] = engineered_df\n",
        "        feature_mappings[ticker] = mapping\n",
        "\n",
        "    return fe_data, feature_mappings\n",
        "\n",
        "# Apply feature engineering to cleaned data\n",
        "feature_data, feature_mappings = apply_feature_engineering_simplified(cleaned_data)\n",
        "\n",
        "print(\"Feature engineering completed!\")\n",
        "display(feature_data.get('TATACONSUM.NS'))\n",
        "display(feature_mappings.get('TATACONSUM.NS'))"
      ],
      "metadata": {
        "id": "n-Mos9NRaIdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERATING TARGET (STATE SPACE MODEL- KALMAN FILTER BASED)"
      ],
      "metadata": {
        "id": "h9HR6F05Gv9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from collections.abc import Iterable\n",
        "def generate_kalman_target(df, feature_mapping,\n",
        "                           undervaluation_threshold,\n",
        "                           confirmation_window,\n",
        "                           trend_requirement=True):\n",
        "    \"\"\"\n",
        "    Generate binary target based on Kalman fair price instead of future price movements.\n",
        "    \"\"\"\n",
        "    # 1. Extract and validate column names\n",
        "    close_col      = feature_mapping.get(\"close\")\n",
        "    fair_price_col = feature_mapping.get(\"kalman_fair_price\")\n",
        "    deviation_col  = feature_mapping.get(\"fair_price_deviation\")\n",
        "\n",
        "\n",
        "    # 2. Work on a copy and drop rows missing key data\n",
        "    df = df.copy()\n",
        "    df = df.dropna(subset=[close_col, fair_price_col, deviation_col])\n",
        "\n",
        "    # ensure we get a Series, not accidentally a DataFrame\n",
        "    raw_dev = df[deviation_col]\n",
        "    if isinstance(raw_dev, pd.DataFrame):\n",
        "        # take the first column if multiple returned\n",
        "        raw_dev = raw_dev.iloc[:, 0]\n",
        "    raw_dev = raw_dev.astype(float)\n",
        "\n",
        "    df['is_undervalued'] = raw_dev <= float(undervaluation_threshold)\n",
        "    df['undervalued_persistent'] = (\n",
        "        df['is_undervalued']\n",
        "          .rolling(window=confirmation_window, min_periods=1)\n",
        "          .sum() >= (confirmation_window // 2)\n",
        "    )\n",
        "    df['undervalued_persistent'] = (\n",
        "        df['is_undervalued']\n",
        "          .rolling(window=confirmation_window, min_periods=1)\n",
        "          .sum() >= (confirmation_window // 2)\n",
        "    )\n",
        "\n",
        "    # 4. Optional trend & vol filters\n",
        "    if trend_requirement:\n",
        "        df['recent_return'] = df[close_col].pct_change(5)\n",
        "        df['trend_ok'] = df['recent_return'] > -0.10\n",
        "\n",
        "        df['volatility'] = df[close_col].pct_change().rolling(window=10).std()\n",
        "        vol_cutoff = df['volatility'].quantile(0.9)\n",
        "        df['volatility_ok'] = df['volatility'] <= vol_cutoff\n",
        "\n",
        "        df['target'] = (\n",
        "            df['undervalued_persistent'] &\n",
        "            df['trend_ok'] &\n",
        "            df['volatility_ok']\n",
        "        ).astype(int)\n",
        "    else:\n",
        "        df['target'] = df['undervalued_persistent'].astype(int)\n",
        "\n",
        "    # 5. Cleanup temporary columns\n",
        "    drop_cols = ['is_undervalued', 'undervalued_persistent']\n",
        "    if trend_requirement:\n",
        "        drop_cols += ['recent_return', 'trend_ok', 'volatility', 'volatility_ok']\n",
        "    df.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
        "\n",
        "    # Move target to end\n",
        "    cols = [c for c in df.columns if c != 'target']\n",
        "    df = df[cols + ['target']]\n",
        "\n",
        "    # Handle NaNs in target\n",
        "    if df['target'].isnull().any():\n",
        "        df['target'].fillna(0, inplace=True)\n",
        "        print(\"Warning: NaNs in 'target' filled with 0.\")\n",
        "\n",
        "    # Diagnostics\n",
        "    dist = df['target'].value_counts().to_dict()\n",
        "    ratio = dist.get(1, 0) / len(df)\n",
        "    # print(f\"Target distribution: {dist}\")\n",
        "    # print(f\"Positive target ratio: {ratio:.3f}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def apply_kalman_target_generation(feature_data_dict, feature_mappings, **kwargs):\n",
        "    \"\"\"\n",
        "    Apply Kalman target generation across all tickers.\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "    for ticker, df in feature_data_dict.items():\n",
        "        try:\n",
        "            print(f\"\\nProcessing {ticker}...\")\n",
        "            mapping = feature_mappings[ticker]\n",
        "            result[ticker] = generate_kalman_target(df, mapping, **kwargs)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed for {ticker}: {e}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "kalman_target_data = apply_kalman_target_generation(\n",
        "    feature_data,\n",
        "    feature_mappings,\n",
        "    undervaluation_threshold=0.985,\n",
        "    confirmation_window=5,\n",
        "    trend_requirement=True\n",
        ")"
      ],
      "metadata": {
        "id": "So523JEeaMtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TARGET BALANCING, HYPERPARAMETER OPTIMIZATION, FEATURE SELECTION, CROSS VALIDATION"
      ],
      "metadata": {
        "id": "wD8DNzgXG7pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Overfitting-Reduced Model Training Pipeline\n",
        "\n",
        "def balance_target_conservative(X, y, sampling_strategy='auto'):\n",
        "    \"\"\"\n",
        "    Conservative target balancing to avoid overfitting\n",
        "    \"\"\"\n",
        "    y = np.array(y)\n",
        "    if y.dtype != np.int64:\n",
        "        y = y.astype(int)\n",
        "\n",
        "    print(f\"Initial class distribution: {np.bincount(y)}\")\n",
        "\n",
        "    # Use less aggressive oversampling\n",
        "    oversampler = SMOTE(random_state=42, sampling_strategy=sampling_strategy)\n",
        "    X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
        "\n",
        "    print(f\"Resampled class distribution: {np.bincount(y_resampled)}\")\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "def train_test_split_timeseries(df, test_size=0.3):\n",
        "    \"\"\"\n",
        "    Time-series aware train-test split (no shuffling)\n",
        "    \"\"\"\n",
        "    X = df.iloc[:, :-1]\n",
        "    y = df.iloc[:, -1]\n",
        "\n",
        "    if X.shape[0] == 0:\n",
        "        raise ValueError(\"Input DataFrame is empty\")\n",
        "\n",
        "    if y.isnull().any():\n",
        "        raise ValueError(\"Target contains NaN values\")\n",
        "\n",
        "    # Time-series split (no random shuffling)\n",
        "    split_idx = int(len(df) * (1 - test_size))\n",
        "    X_train = X.iloc[:split_idx]\n",
        "    X_test = X.iloc[split_idx:]\n",
        "    y_train = y.iloc[:split_idx]\n",
        "    y_test = y.iloc[split_idx:]\n",
        "\n",
        "    print(f\"Train set: {X_train.shape[0]} samples\")\n",
        "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "    print(f\"Train period: {X_train.index[0]} to {X_train.index[-1]}\")\n",
        "    print(f\"Test period: {X_test.index[0]} to {X_test.index[-1]}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def optimize_hyperparameters_conservative(X_train, y_train, n_trials=30):\n",
        "    \"\"\"\n",
        "    Conservative hyperparameter optimization to reduce overfitting\n",
        "    \"\"\"\n",
        "    def objective(trial):\n",
        "        # More conservative parameter ranges\n",
        "        params = {\n",
        "            \"objective\": \"binary:logistic\",\n",
        "            \"booster\": \"gbtree\",\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 150),  # Reduced from 200\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15),  # Reduced from 0.3\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 6),  # Reduced from 12\n",
        "            \"gamma\": trial.suggest_float(\"gamma\", 0.1, 2.0),  # Increased minimum\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 0.9),  # More conservative\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 0.9),  # More conservative\n",
        "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 10.0),  # Increased regularization\n",
        "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 5.0),  # Increased regularization\n",
        "            \"random_state\": 42\n",
        "            # \"eval_metric\": \"logloss\"\n",
        "        }\n",
        "\n",
        "        # Use TimeSeriesSplit for validation\n",
        "        tscv = TimeSeriesSplit(n_splits=3)\n",
        "        scores = []\n",
        "\n",
        "        for train_idx, val_idx in tscv.split(X_train):\n",
        "            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "            model = XGBClassifier(**params)\n",
        "            model.fit(X_tr, y_tr)\n",
        "            score = model.score(X_val, y_val)\n",
        "            scores.append(score)\n",
        "\n",
        "        return np.mean(scores)\n",
        "\n",
        "    study = create_study(direction='maximize', sampler=TPESampler())\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "    print(f\"Best validation score: {study.best_value:.4f}\")\n",
        "    return study.best_params\n",
        "\n",
        "\n",
        "def select_important_features_conservative(X_train, y_train, max_features=25):\n",
        "    # 0) Identify & drop any columns with list/array cells\n",
        "    ragged = [\n",
        "        c for c in X_train.columns\n",
        "        if X_train[c].apply(lambda x: isinstance(x, (list, np.ndarray))).any()\n",
        "    ]\n",
        "    if ragged:\n",
        "        print(\"Dropping ragged columns:\", ragged)\n",
        "    X_clean = X_train.drop(columns=ragged)\n",
        "\n",
        "    # 1) Convert to numpy\n",
        "    X_num = X_clean.astype(np.float64).to_numpy()\n",
        "    y     = np.asarray(y_train)\n",
        "\n",
        "    # 2) Train a regularized XGBoost\n",
        "    model = XGBClassifier(\n",
        "        objective=\"binary:logistic\",\n",
        "        n_estimators=100,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.1,\n",
        "        reg_lambda=2.0,\n",
        "        reg_alpha=1.0,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_num, y)\n",
        "\n",
        "    # 3) Rehydrate importances into pandas\n",
        "    feature_names = list(X_clean.columns)\n",
        "    imp = pd.Series(model.feature_importances_, index=feature_names) \\\n",
        "             .sort_values(ascending=False)\n",
        "\n",
        "    # 4) Pick top-N\n",
        "    top_feats = imp.nlargest(max_features).index.tolist()\n",
        "    selected_idx = [feature_names.index(f) for f in top_feats]\n",
        "\n",
        "    print(\n",
        "        f\"Selected {len(selected_idx)} features \"\n",
        "        f\"out of {X_num.shape[1]} numeric inputs.\"\n",
        "    )\n",
        "\n",
        "    # 5) Plot\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    imp.nlargest(15).plot.barh()\n",
        "    plt.title(\"Top 15 Feature Importances\")\n",
        "    plt.xlabel(\"Importance\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return selected_idx\n",
        "\n",
        "def train_conservative_model(X_train, y_train, X_val, y_val, best_params):\n",
        "    \"\"\"\n",
        "    Train model with enhanced overfitting protection\n",
        "    \"\"\"\n",
        "    # Add additional conservative settings\n",
        "    conservative_params = best_params.copy()\n",
        "    conservative_params.update({\n",
        "        'eval_metric': 'logloss',\n",
        "        'early_stopping_rounds': 15,  # More aggressive early stopping\n",
        "        'verbose': False\n",
        "    })\n",
        "\n",
        "    # Prepare DMatrix with feature names for better interpretability\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=X_train.columns.tolist())\n",
        "    dval = xgb.DMatrix(X_val, label=y_val, feature_names=X_train.columns.tolist())\n",
        "\n",
        "    # Train with early stopping\n",
        "    model = xgb.train(\n",
        "        params=conservative_params,\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=300,  # Reduced from 500\n",
        "        evals=[(dtrain, \"train\"), (dval, \"eval\")],\n",
        "        early_stopping_rounds=15,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "\n",
        "    print(f\"Training stopped at iteration: {model.best_iteration}\")\n",
        "    print(f\"Best validation score: {model.best_score:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def cross_validate_conservative(X, y, n_splits=5):\n",
        "    \"\"\"\n",
        "    Conservative cross-validation using TimeSeriesSplit\n",
        "    \"\"\"\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "    scores = []\n",
        "\n",
        "    for train_idx, val_idx in tscv.split(X):\n",
        "        X_train_cv = X.iloc[train_idx]\n",
        "        X_val_cv = X.iloc[val_idx]\n",
        "        y_train_cv = y.iloc[train_idx]\n",
        "        y_val_cv = y.iloc[val_idx]\n",
        "\n",
        "        # Simple model for CV\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=4,\n",
        "            learning_rate=0.1,\n",
        "            reg_lambda=2.0,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        model.fit(X_train_cv, y_train_cv)\n",
        "        score = model.score(X_val_cv, y_val_cv)\n",
        "        scores.append(score)\n",
        "\n",
        "    print(f\"Cross-validation scores: {[f'{s:.3f}' for s in scores]}\")\n",
        "    print(f\"Mean CV score: {np.mean(scores):.3f} (+/- {np.std(scores)*2:.3f})\")\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "print(\" training pipeline functions defined!\")"
      ],
      "metadata": {
        "id": "VyNVr-MpaXcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING, TESTING, EVALUATION"
      ],
      "metadata": {
        "id": "4BiN47cfHQOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7: Main Execution Pipeline (Conservative Approach)\n",
        "\n",
        "def execute_conservative_pipeline(df, ticker_name):\n",
        "    \"\"\"\n",
        "    Execute the complete conservative model pipeline\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"EXECUTING CONSERVATIVE PIPELINE FOR {ticker_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Step 1: Time-series aware train-test split\n",
        "    print(\"\\n1. Time-series Train-Test Split...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split_timeseries(\n",
        "        df, test_size=0.3\n",
        "    )\n",
        "\n",
        "    # Step 2: Conservative target balancing\n",
        "    print(\"\\n2. Conservative Target Balancing...\")\n",
        "    X_train_balanced, y_train_balanced = balance_target_conservative(X_train, y_train)\n",
        "\n",
        "    # Step 3: Train-validation split for model selection\n",
        "    print(\"\\n3. Creating Validation Set...\")\n",
        "    val_size = int(0.2 * len(X_train_balanced))\n",
        "    X_train_final = X_train_balanced[:-val_size]\n",
        "    X_val = X_train_balanced[-val_size:]\n",
        "    y_train_final = y_train_balanced[:-val_size]\n",
        "    y_val = y_train_balanced[-val_size:]\n",
        "\n",
        "    # Convert to DataFrame for consistency\n",
        "    X_train_final = pd.DataFrame(X_train_final, columns=X_train.columns)\n",
        "    X_val = pd.DataFrame(X_val, columns=X_train.columns)\n",
        "    y_train_final = pd.Series(y_train_final)\n",
        "    y_val = pd.Series(y_val)\n",
        "\n",
        "    print(f\"Final train set: {len(X_train_final)} samples\")\n",
        "    print(f\"Validation set: {len(X_val)} samples\")\n",
        "\n",
        "    # Step 4: Conservative feature selection\n",
        "    print(\"\\n4. Conservative Feature Selection...\")\n",
        "    important_features_idx = select_important_features_conservative(\n",
        "        X_train_final, y_train_final, max_features=20  # Reduced from 25\n",
        "    )\n",
        "\n",
        "    # Apply feature selection\n",
        "    feature_names = X_train_final.columns[important_features_idx].tolist()\n",
        "    X_train_selected = X_train_final.iloc[:, important_features_idx]\n",
        "    X_val_selected = X_val.iloc[:, important_features_idx]\n",
        "    X_test_selected = X_test.iloc[:, important_features_idx]\n",
        "\n",
        "    print(f\"Selected features: {feature_names}\")\n",
        "\n",
        "    # Step 5: Conservative hyperparameter optimization\n",
        "    print(\"\\n5. Conservative Hyperparameter Optimization...\")\n",
        "    best_params = optimize_hyperparameters_conservative(\n",
        "        X_train_selected, y_train_final, n_trials=30  # Reduced trials\n",
        "    )\n",
        "    print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "    training_feature_names = list(X_train_selected.columns)\n",
        "\n",
        "    # Step 6: Train final model\n",
        "    print(\"\\n6. Training Conservative Model...\")\n",
        "    final_model = train_conservative_model(\n",
        "        X_train_selected, y_train_final, X_val_selected, y_val, best_params\n",
        "    )\n",
        "\n",
        "    # Step 7: Model evaluation\n",
        "    print(\"\\n7. Model Evaluation...\")\n",
        "\n",
        "    # Validation predictions\n",
        "    dval = xgb.DMatrix(X_val_selected, feature_names=feature_names)\n",
        "    y_val_proba = final_model.predict(dval)\n",
        "    y_val_pred = (y_val_proba > 0.5).astype(int)\n",
        "\n",
        "    # Test predictions\n",
        "    dtest = xgb.DMatrix(X_test_selected, feature_names=feature_names)\n",
        "    y_test_proba = final_model.predict(dtest)\n",
        "    y_test_pred = (y_test_proba > 0.5).astype(int)\n",
        "\n",
        "    # Training predictions\n",
        "    dtrain = xgb.DMatrix(X_train_selected, feature_names=feature_names)\n",
        "    y_train_proba = final_model.predict(dtrain)\n",
        "    y_train_pred = (y_train_proba > 0.5).astype(int)\n",
        "\n",
        "    # Print performance metrics\n",
        "\n",
        "    print(\"\\nTEST PERFORMANCE:\")\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "    print(\"\\nTRAIN PERFORMANCE:\")\n",
        "    print(classification_report(y_train_final, y_train_pred))\n",
        "\n",
        "    # Calculate overfitting indicators\n",
        "    train_acc = accuracy_score(y_train_final, y_train_pred)\n",
        "    val_acc = accuracy_score(y_val, y_val_pred)\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    overfitting_ratio = (train_acc - test_acc) / train_acc\n",
        "\n",
        "    print(f\"\\nOVERFITTING ANALYSIS:\")\n",
        "    print(f\"Train Accuracy: {train_acc:.3f}\")\n",
        "    print(f\"Validation Accuracy: {val_acc:.3f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.3f}\")\n",
        "    print(f\"Overfitting Ratio: {overfitting_ratio:.3f} ({'GOOD' if overfitting_ratio < 0.1 else 'POOR'})\")\n",
        "\n",
        "    # Step 8: Cross-validation\n",
        "    print(\"\\n8. Cross-Validation Assessment...\")\n",
        "    cv_score = cross_validate_conservative(X_train_selected, y_train_final)\n",
        "\n",
        "    # Visualizations\n",
        "    print(\"\\n9. Generating Visualizations...\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n",
        "    plt.title(f\"Confusion Matrix - {ticker_name}\")\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curve\n",
        "    fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\n",
        "    roc_auc_test = auc(fpr_test, tpr_test)\n",
        "\n",
        "    fpr_train, tpr_train, _ = roc_curve(y_train_final, y_train_proba)\n",
        "    roc_auc_train = auc(fpr_train, tpr_train)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr_train, tpr_train, label=f\"Train ROC (AUC = {roc_auc_train:.3f})\")\n",
        "    plt.plot(fpr_test, tpr_test, label=f\"Test ROC (AUC = {roc_auc_test:.3f})\")\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Random\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(f\"ROC Curves - {ticker_name}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Return results\n",
        "    results = {\n",
        "        'model': final_model,\n",
        "        'best_params': best_params,\n",
        "        'feature_names': feature_names,\n",
        "        'train_accuracy': train_acc,\n",
        "        'validation_accuracy': val_acc,\n",
        "        'test_accuracy': test_acc,\n",
        "        'cv_score': cv_score,\n",
        "        'overfitting_ratio': overfitting_ratio,\n",
        "        'roc_auc_train': roc_auc_train,\n",
        "        'roc_auc_test': roc_auc_test,\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_conservative_pipeline_all_stocks(target_data_dict):\n",
        "    \"\"\"\n",
        "    Run conservative pipeline for all stocks\n",
        "    \"\"\"\n",
        "    all_results = {}\n",
        "\n",
        "    for ticker, df in target_data_dict.items():\n",
        "        print(f\"\\n\\nProcessing {ticker}...\")\n",
        "\n",
        "        try:\n",
        "            results = execute_conservative_pipeline(df, ticker)\n",
        "            all_results[ticker] = results\n",
        "\n",
        "            print(f\"\\n{ticker} SUMMARY:\")\n",
        "            print(f\"Final Test Accuracy: {results['test_accuracy']:.3f}\")\n",
        "            print(f\"Overfitting Ratio: {results['overfitting_ratio']:.3f}\")\n",
        "            print(f\"Cross-Validation Score: {results['cv_score']:.3f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {ticker}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# Execute the conservative pipeline\n",
        "model_results = run_conservative_pipeline_all_stocks(kalman_target_data)"
      ],
      "metadata": {
        "id": "ZpSzO7ozaeaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIVE PREDICTIONS TO GET A BUY LIST OF STOCKS"
      ],
      "metadata": {
        "id": "AcUHBFyxHbFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model, X, feature_names=None):\n",
        "    \"\"\"Ensure feature alignment before prediction and drop duplicate columns.\"\"\"\n",
        "    # 1) Align to expected feature list if provided\n",
        "    if feature_names is not None:\n",
        "        missing = set(feature_names) - set(X.columns)\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing features in prediction data: {missing}\")\n",
        "        # enforce order, drop names not in feature_names\n",
        "        X = X.loc[:, [f for f in feature_names if f in X.columns]]\n",
        "\n",
        "    # 2) Drop duplicate columns (keep first)\n",
        "    X = X.loc[:, ~X.columns.duplicated()]\n",
        "\n",
        "    # 3) Use the final column list as the DMatrix feature names\n",
        "    final_feat_names = list(X.columns)\n",
        "\n",
        "    # 4) Convert to numpy and build DMatrix\n",
        "    X_numeric = X.astype(np.float64).to_numpy()\n",
        "    dmatrix = xgb.DMatrix(X_numeric, feature_names=final_feat_names)\n",
        "\n",
        "    # 5) Predict\n",
        "    y_pred_proba = model.predict(dmatrix)\n",
        "    y_pred       = (y_pred_proba > 0.5).astype(int)\n",
        "    return y_pred_proba, y_pred"
      ],
      "metadata": {
        "id": "QfehqdKZ0ecG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "live_data_dict = {}\n",
        "\n",
        "for ticker in get_nifty50_tickers():\n",
        "    try:\n",
        "        data = yf.download(ticker, period=\"400d\", interval=\"1d\", progress=False)\n",
        "\n",
        "        data = data.rename(columns={\n",
        "            'Open': 'open',\n",
        "            'High': 'high',\n",
        "            'Low': 'low',\n",
        "            'Close': 'close',\n",
        "            'Adj Close': 'adj_close',\n",
        "            'Volume': 'volume'\n",
        "        }).reset_index()\n",
        "\n",
        "        data['turnover'] = data['volume'] * data['close']\n",
        "\n",
        "        data = data[['Date', 'open', 'high', 'low', 'close', 'volume', 'turnover']]\n",
        "        data = data.rename(columns={'Date': 'timestamp'})\n",
        "\n",
        "        live_data_dict[ticker] = data.dropna()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process live data for {ticker}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "# print(f\"\\nCollected live data for {len(live_data_dict)} stocks\")\n",
        "# print(\"Sample live data for CANBK.NS (showing the last few rows to verify latest signal):\")\n",
        "# display(live_data_dict.get('CANBK.NS', pd.DataFrame()).tail())\n",
        "\n",
        "cleaned_data_2 = clean_stock_data(live_data_dict)\n",
        "\n",
        "# display(cleaned_data_2.get('POWERGRID.NS', pd.DataFrame()).tail())\n",
        "\n",
        "\n",
        "\n",
        "def live_prediction(live_data_dict, model_dict):\n",
        "    \"\"\"\n",
        "    Process live data for each stock to get a prediction on the most recent datapoint.\n",
        "\n",
        "    For each stock in live_data_dict:\n",
        "      1. Ensure the DataFrame contains at least 50 data points (sorted by timestamp).\n",
        "      2. Take the last 50 datapoints and apply feature engineering.\n",
        "      3. Generate target features using generate_simple_target with target_pct=0.03 and drawdown_limit=0.98.\n",
        "      4. Extract the latest row and align its features with the training features from model_dict.\n",
        "      5. Use the stored XGBoost model to predict the probability and binary target.\n",
        "      6. If the predicted label is 1, add the ticker to the buy list.\n",
        "\n",
        "    Returns:\n",
        "      buy_list: List of tickers with a BUY signal.\n",
        "      predictions: Dictionary with ticker as key and (predicted probability, predicted label) as value.\n",
        "    \"\"\"\n",
        "    buy_list = []\n",
        "    predictions = {}\n",
        "\n",
        "    for ticker, live_df in live_data_dict.items():\n",
        "\n",
        "        if len(live_df) < 50:\n",
        "            print(f\"Not enough data for {ticker}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        live_df = live_df.sort_index()\n",
        "        live_df_recent = live_df.iloc[-380:].copy()\n",
        "\n",
        "        fe_live_df, live_mapping = feature_engineering_simplified(live_df_recent)\n",
        "        live_df_with_target = generate_kalman_target(fe_live_df, live_mapping, undervaluation_threshold=0.985, confirmation_window=5, trend_requirement=True)\n",
        "\n",
        "        latest_row = live_df_with_target.iloc[[-1]]\n",
        "\n",
        "        if ticker not in model_dict:\n",
        "            print(f\"Model for {ticker} not found. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        training_features = model_dict[ticker][\"feature_names\"]\n",
        "\n",
        "        X_live = latest_row[training_features]\n",
        "\n",
        "        pred_proba, pred = make_predictions(model_dict[ticker][\"model\"], X_live, feature_names=training_features)\n",
        "        predictions[ticker] = (pred_proba[0], pred[0])\n",
        "\n",
        "        if pred[0] == 1:\n",
        "            buy_list.append(ticker)\n",
        "            print(f\"{ticker}: BUY signal (predicted probability: {pred_proba[0]:.2f})\")\n",
        "        else:\n",
        "            print(f\"{ticker}: NO BUY signal (predicted probability: {pred_proba[0]:.2f})\")\n",
        "\n",
        "\n",
        "    return buy_list, predictions\n",
        "\n",
        "\n",
        "buy_list, live_predictions = live_prediction(cleaned_data_2, model_results)"
      ],
      "metadata": {
        "id": "BQ49GhMPzcy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buy_with_probs = {\n",
        "    ticker: live_predictions[ticker][0]\n",
        "    for ticker in buy_list\n",
        "}\n",
        "\n",
        "# Sort by probability, highest first\n",
        "sorted_buys = sorted(\n",
        "    buy_with_probs.items(),\n",
        "    key=lambda kv: kv[1],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "print(\"\\n=== BUY LIST (sorted by predicted probability) ===\")\n",
        "for ticker, prob in sorted_buys:\n",
        "    print(f\"{ticker:10s} →  {prob*100:5.1f}%\")"
      ],
      "metadata": {
        "id": "y5GlhZAp_eTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ——— 2) Enrich each buy with price/fair‐value info and signal strength ———\n",
        "enriched_predictions = {}\n",
        "final_buy_signals = []\n",
        "\n",
        "for ticker, prob in sorted_buys:\n",
        "    # grab the latest features DataFrame you used for prediction\n",
        "    # if you need to recompute it, re‑run feature engineering on the last 1 row:\n",
        "    latest_features = generate_kalman_target(\n",
        "        *feature_engineering_simplified(live_data_dict[ticker].iloc[-380:]) , undervaluation_threshold = 0.985 , confirmation_window = 5 , trend_requirement = True\n",
        "    ).iloc[[-1]]\n",
        "\n",
        "    # fetch the mapping you used\n",
        "    _, live_mapping = feature_engineering_simplified(cleaned_data_2[ticker].iloc[-380:])\n",
        "    close_col      = live_mapping[\"close\"]\n",
        "    fair_price_col = live_mapping[\"kalman_fair_price\"]\n",
        "    deviation_col  = live_mapping[\"fair_price_deviation\"]\n",
        "\n",
        "\n",
        "    # Current price\n",
        "    raw_curr = latest_features[close_col]\n",
        "    if isinstance(raw_curr, pd.DataFrame):\n",
        "        raw_curr = raw_curr.iloc[:, 0]\n",
        "    current_price = float(raw_curr.iat[0])\n",
        "\n",
        "    # Fair price\n",
        "    raw_fair = latest_features[fair_price_col]\n",
        "    if isinstance(raw_fair, pd.DataFrame):\n",
        "        raw_fair = raw_fair.iloc[:, 0]\n",
        "    fair_price = float(raw_fair.iat[0])\n",
        "\n",
        "    # Deviation\n",
        "    raw_dev = latest_features[deviation_col]\n",
        "    if isinstance(raw_dev, pd.DataFrame):\n",
        "        raw_dev = raw_dev.iloc[:, 0]\n",
        "    deviation = float(raw_dev.iat[0])\n",
        "\n",
        "    # compute true percent deviation\n",
        "    deviation_pct = (fair_price - current_price) / current_price * 100\n",
        "\n",
        "    # binary prediction label\n",
        "    _, pred = live_predictions[ticker]\n",
        "\n",
        "    enriched_predictions[ticker] = {\n",
        "        'probability':     prob,\n",
        "        'prediction':      pred,\n",
        "        'current_price':   current_price,\n",
        "        'fair_price':      fair_price,\n",
        "        'deviation_pct':   deviation_pct,\n",
        "        'is_undervalued':  deviation_pct >= 2.0,\n",
        "        'signal_strength': 'STRONG' if prob > 0.7 else 'WEAK' if prob > 0.5 else 'NONE'\n",
        "    }\n",
        "\n",
        "    if pred == 1:\n",
        "        final_buy_signals.append(ticker)\n",
        "        print(f\"\\n🟢 BUY SIGNAL for {ticker}\")\n",
        "        print(f\"   Probability    : {prob:.3f}\")\n",
        "        print(f\"   Current Price  : ₹{current_price:.2f}\")\n",
        "        print(f\"   Fair Price     : ₹{fair_price:.2f}\")\n",
        "        # Now deviation is a float, so this will format cleanly\n",
        "        print(f\"   Undervaluation : {deviation_pct:.1f}% ({'YES' if deviation_pct >= 2.0 else 'NO'})\")\n",
        "        print(f\"   Signal Strength: {enriched_predictions[ticker]['signal_strength']}\")\n",
        "    else:\n",
        "        print(f\"\\n🔴 NO BUY signal for {ticker} (prob: {prob:.3f})\")\n",
        "\n",
        "\n",
        "# ——— 3) Generate trading recommendations ———\n",
        "def generate_trading_recommendations(buy_signals, predictions, capital=100000):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRADING RECOMMENDATIONS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if not buy_signals:\n",
        "        print(\"❌ No buy signals generated.\")\n",
        "        return\n",
        "\n",
        "    print(f\"💰 Total Capital: ₹{capital:,.0f}\")\n",
        "    print(f\"📈 Buy Signals: {len(buy_signals)} stocks\")\n",
        "    position_per_stock = capital / len(buy_signals)\n",
        "\n",
        "    for ticker in buy_signals:\n",
        "        info = predictions[ticker]\n",
        "        cp = info['current_price']\n",
        "        shares = int(position_per_stock / cp)\n",
        "        position_value = shares * cp\n",
        "        t1 = cp * 1.05\n",
        "        t2 = cp * 1.10\n",
        "        sl = cp * 0.95\n",
        "\n",
        "        print(f\"\\n🏢 {ticker}\")\n",
        "        print(f\"   Signal Strength: {info['signal_strength']}\")\n",
        "        print(f\"   Probability    : {info['probability']:.3f}\")\n",
        "        print(f\"   Current Price  : ₹{cp:.2f}\")\n",
        "        print(f\"   Fair Price     : ₹{info['fair_price']:.2f}\")\n",
        "        print(f\"   Undervaluation : {info['deviation_pct']:.1f}%\")\n",
        "        print(f\"   Shares to Buy  : {shares:,}\")\n",
        "        print(f\"   Position Value : ₹{position_value:,.0f}\")\n",
        "        print(f\"   Target 1 (5%)  : ₹{t1:.2f}\")\n",
        "        print(f\"   Target 2 (10%) : ₹{t2:.2f}\")\n",
        "        print(f\"   Stop Loss (5%): ₹{sl:.2f}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# finally call it:\n",
        "capital = float(input(\"\\nEnter trading capital (₹): \") or \"100000\")\n",
        "generate_trading_recommendations(final_buy_signals, enriched_predictions, capital)"
      ],
      "metadata": {
        "id": "8o6ewlS9auGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KALMAN FILTER (A LINEAR STATE SPACE MODEL) BASED FAIR PRICE ESTIMATION OF STOCK IN BUY LIST WHICH TELLS IF THE STOCK IS OVERVALUED OR UNDERVALUED."
      ],
      "metadata": {
        "id": "GN5KNmB4F16o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXAMPLE TAKEN- TATACONSUM.NS\n",
        "\n",
        "RE-RUN THE CODE TO HAVE THE PLOT, METRICS, FAIR PRICE FOR EVERY STOCK IN THE BUY LIST"
      ],
      "metadata": {
        "id": "D_wGccZHHskB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "signals = {}\n",
        "rmse_vals = []\n",
        "mae_vals  = []\n",
        "plotted_tickers = []\n",
        "\n",
        "for ticker in buy_list:\n",
        "    if ticker not in cleaned_data_2:\n",
        "        continue\n",
        "\n",
        "    # Prepare data\n",
        "    df = cleaned_data_2[ticker].copy()\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df['log_close'] = np.log(df['close'])\n",
        "\n",
        "    # Kalman filter on log-prices\n",
        "    kf = KalmanFilter(\n",
        "        initial_state_mean=df['log_close'].iloc[0],\n",
        "        n_dim_obs=1,\n",
        "        n_dim_state=1\n",
        "    )\n",
        "    state_means, _ = kf.filter(df['log_close'].values.reshape(-1,1))\n",
        "    df['fair_price'] = np.exp(state_means.flatten())\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(df['close'], df['fair_price']))\n",
        "    mae  = mean_absolute_error(df['close'], df['fair_price'])\n",
        "    rmse_vals.append(rmse)\n",
        "    mae_vals.append(mae)\n",
        "    plotted_tickers.append(ticker)\n",
        "\n",
        "\n",
        "    actual_last = df['close'].iloc[-1]\n",
        "    fair_last   = df['fair_price'].iloc[-1]\n",
        "\n",
        "    #\n",
        "    if actual_last < fair_last:\n",
        "        status = \"UNDERVALUED\"\n",
        "    elif actual_last > fair_last:\n",
        "        status = \"OVERVALUED\"\n",
        "    else:\n",
        "        status = \"FAIRLY VALUED\"\n",
        "\n",
        "    # Trading signal: buy only if undervalued\n",
        "    action = \"BUY\" if status == \"UNDERVALUED\" else \"HOLD\"\n",
        "\n",
        "    signals[ticker] = (status, action, actual_last, fair_last, rmse, mae)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(df.index, df['close'],      label='Actual Close',     linewidth=2)\n",
        "    plt.plot(df.index, df['fair_price'], label='Kalman Fair Price', linewidth=2)\n",
        "    plt.title(f\"{ticker}: Actual vs. Kalman Fair Price\")\n",
        "    plt.xlabel(\"Date\"); plt.ylabel(\"Price\")\n",
        "    plt.legend(); plt.grid(True); plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for ticker, (status, action, actual, fair, rmse, mae) in signals.items():\n",
        "    print(f\"{ticker}: LAST CLOSE = {actual:.2f}, FAIR PRICE = {fair:.2f}\")\n",
        "    print(f\"    STATUS = {status} → {action}\")\n",
        "    print(f\"    RMSE = {rmse:.4f}, MAE = {mae:.4f}\\n\")"
      ],
      "metadata": {
        "id": "r59dqzvSEpRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BACKTEST OF THE STRATEGY\n",
        "(WITH VOLATILITY SCALING/TARGETTING)"
      ],
      "metadata": {
        "id": "uzrHaRgkFfs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "THE STRATEGY-\n",
        "\n",
        "STOCKS TO TRADE IN- WHEN WE GET THE BUY SIDE BIASED STOCK LIST,\n",
        "\n",
        "ENTRY- WE LOOK FOR FAIR PRICE VALUATION AND IF UNDERVALUED WE BUY\n",
        "\n",
        "EXIT- EXIT POSITION WHEN EITHER\n",
        "    \n",
        "    A) TARGET OF 15% IS MET\n",
        "\n",
        "    OR B) SL OF 5% IS MET"
      ],
      "metadata": {
        "id": "ODEB-M89FqKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXAMPLE TAKEN- TATACONSUM.NS\n",
        "\n",
        "RE-RUN THE CODE TO HAVE THE PLOT, METRICS, FAIR PRICE FOR EVERY STOCK IN THE BUY LIST"
      ],
      "metadata": {
        "id": "6_AEQ620IGPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_2 = {}\n",
        "for ticker in buy_list:\n",
        "    df = yf.download(ticker, period=\"400d\", interval=\"1d\", progress=False)\n",
        "    df = df.rename(columns={\n",
        "        'Open':'open','High':'high','Low':'low','Close':'close',\n",
        "        'Adj Close':'adj_close','Volume':'volume'\n",
        "    }).reset_index()\n",
        "    df['turnover'] = df['volume'] * df['close']\n",
        "    df = df[['Date','open','high','low','close','volume','turnover']]\n",
        "    df.rename(columns={'Date':'timestamp'}, inplace=True)\n",
        "    data_2[ticker] = df.dropna().reset_index(drop=True)\n",
        "\n",
        "cleaned_data_2= clean_stock_data(data_2)\n",
        "\n",
        "window = 380\n",
        "target_pct = 0.15\n",
        "drawdown_limit = 0.95\n",
        "\n",
        "for ticker in buy_list:\n",
        "    df = cleaned_data_2[ticker].copy()\n",
        "    if len(df) < window:\n",
        "        print(f\"{ticker}: not enough history ({len(df)} < {window})\")\n",
        "        continue\n",
        "\n",
        "    live_df = df.iloc[-window:].copy()\n",
        "    live_df = live_df.sort_index()\n",
        "\n",
        "    sig_data = pd.DataFrame(index=live_df.index)\n",
        "\n",
        "\n",
        "    # Kalman-fair price for each of the window days\n",
        "    live_df['log_close'] = np.log(live_df['close'])\n",
        "    kf = KalmanFilter(\n",
        "        initial_state_mean=live_df['log_close'].iloc[0],\n",
        "        n_dim_obs=1, n_dim_state=1\n",
        "    )\n",
        "    state_means, _ = kf.filter(\n",
        "        live_df['log_close'].values.reshape(-1,1)\n",
        "    )\n",
        "    sig_data['fair_price'] = np.exp(state_means.flatten())\n",
        "    sig_data['close']      = live_df['close'].values\n",
        "\n",
        "    # initial strat_signal\n",
        "    sig_data['strat_signal'] = np.where(\n",
        "        (sig_data['close'] < sig_data['fair_price']),\n",
        "        1, 0\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # carry‐forward until target or SL\n",
        "    updated = []\n",
        "    in_trade = False\n",
        "    entry_price = np.nan\n",
        "\n",
        "    for i in sig_data.index:\n",
        "        if not in_trade and sig_data.at[i, 'strat_signal'] == 1:\n",
        "            in_trade = True\n",
        "            entry_price = sig_data.at[i, 'close']\n",
        "            updated.append(1)\n",
        "        elif in_trade:\n",
        "            price = sig_data.at[i, 'close']\n",
        "            if price >= (1 + target_pct) * entry_price or price <= drawdown_limit * entry_price:\n",
        "                in_trade = False\n",
        "                updated.append(0)\n",
        "            else:\n",
        "                updated.append(1)\n",
        "        else:\n",
        "            updated.append(0)\n",
        "\n",
        "    sig_data['strat_signal'] = updated\n",
        "\n",
        "    # # Shift the signal by 1 to avoid lookahead bias\n",
        "    sig_data['strat_signal'] = sig_data['strat_signal'].shift(1).fillna(0).astype(int)\n",
        "\n",
        "\n",
        "    # strategy returns\n",
        "    returns_series = live_df['close'].pct_change().fillna(0).values\n",
        "    strat_returns = pd.Series(\n",
        "        sig_data['strat_signal'].values * returns_series,\n",
        "        index=live_df.index\n",
        "    )\n",
        "\n",
        "    # Benchmark returns\n",
        "    bench_strat = df[\"close\"].pct_change()\n",
        "    bench_strat.name = \"returns\"\n",
        "    bench_strat = bench_strat.to_frame()  # now you can use ['returns']\n",
        "    bench_strat.dropna(inplace=True)\n",
        "\n",
        "\n",
        "    # VOLATILITY TARGETTING FOR PORTFOLIO OPTIMIZATION\n",
        "    tgt_vol = 0.15\n",
        "    signallls = pd.DataFrame()\n",
        "    signallls['stdev'] = (bench_strat['returns']).rolling(22).std() * np.sqrt(252)  # Convert to annualized standard deviation\n",
        "    signallls['vol_tgt'] = tgt_vol / signallls['stdev']\n",
        "    # Here we use 2x leverage to make sure we can hit our volatility target of 15%\n",
        "    signallls['vol_tgt'] = signallls['vol_tgt'].clip(0, 2)\n",
        "    volatility_tp = signallls.iloc[-380:]\n",
        "\n",
        "    vol_signal = sig_data['strat_signal'].values * volatility_tp['vol_tgt']\n",
        "\n",
        "    vol_strat_returns = pd.Series(\n",
        "        vol_signal.values * returns_series,\n",
        "        index=live_df.index\n",
        "    )\n",
        "\n",
        "    # # g) backtest\n",
        "    qs.reports.metrics(\n",
        "        returns=vol_strat_returns,\n",
        "        benchmark=bench_strat,\n",
        "        mode='full'\n",
        "    )"
      ],
      "metadata": {
        "id": "S5DXuW1NE_7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INTERACTIVE CELL BELOW**"
      ],
      "metadata": {
        "id": "ZOUn94CiA-A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Widget: dropdown for stock selection\n",
        "ticker_dropdown = widgets.Dropdown(\n",
        "    options=get_nifty50_tickers(),\n",
        "    description='Stock:',\n",
        "    value='TATACONSUM.NS'\n",
        ")\n",
        "\n",
        "# Widget: capital input\n",
        "capital_input = widgets.IntText(\n",
        "    value=100000,\n",
        "    description='Capital:'\n",
        ")\n",
        "\n",
        "# Run button\n",
        "run_button = widgets.Button(description=\"Get Recommendation\")\n",
        "\n",
        "# Output area\n",
        "output = widgets.Output()\n",
        "\n",
        "# Function: what happens when the button is clicked\n",
        "def on_click_run(button):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        ticker = ticker_dropdown.value\n",
        "        capital = capital_input.value\n",
        "\n",
        "        print(f\"🔍 Running prediction for: {ticker} with ₹{capital:,} capital\\n\")\n",
        "\n",
        "        try:\n",
        "            live_df = cleaned_data_2[ticker].copy()\n",
        "            fe_df, mapping = feature_engineering_simplified(live_df)\n",
        "            df_with_target = generate_kalman_target(fe_df, mapping, 0.985, 5)\n",
        "\n",
        "            model = model_results[ticker][\"model\"]\n",
        "            features = model_results[ticker][\"feature_names\"]\n",
        "\n",
        "            latest = df_with_target.iloc[[-1]][features]\n",
        "            prob, pred = make_predictions(model, latest, features)\n",
        "\n",
        "            decision = \"🟢 BUY\" if pred[0] == 1 else \"🔴 HOLD\"\n",
        "            print(f\"Prediction: {decision}\")\n",
        "            print(f\"Probability: {prob[0]:.2%}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"⚠️ Error:\", str(e))\n",
        "\n",
        "# Connect the button to the function\n",
        "run_button.on_click(on_click_run)\n",
        "\n",
        "# Display all widgets\n",
        "display(ticker_dropdown, capital_input, run_button, output)"
      ],
      "metadata": {
        "id": "kDxH2e_XMq8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRADIO DASHBOARDS FOR TRIAL**"
      ],
      "metadata": {
        "id": "nrpJR4MEBLll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "Q_gN7ereOxPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Prediction function\n",
        "def predict_signal(ticker, capital):\n",
        "    try:\n",
        "        df = cleaned_data_2[ticker]\n",
        "        fe_df, mapping = feature_engineering_simplified(df)\n",
        "        df_with_target = generate_kalman_target(fe_df, mapping, 0.985, 5)\n",
        "\n",
        "        model = model_results[ticker][\"model\"]\n",
        "        features = model_results[ticker][\"feature_names\"]\n",
        "\n",
        "        latest = df_with_target.iloc[[-1]][features]\n",
        "        prob, pred = make_predictions(model, latest, features)\n",
        "\n",
        "        decision = \"🟢 BUY\" if pred[0] == 1 else \"🔴 HOLD\"\n",
        "        return (\n",
        "            f\"Stock: {ticker}\\n\"\n",
        "            f\"Prediction: {decision}\\n\"\n",
        "            f\"Probability: {prob[0]*100:.2f}%\\n\"\n",
        "            f\"Capital Suggested: ₹{capital:,}\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ INFORMATION UNAVAILABLE {str(e)}\"\n",
        "\n",
        "# Build the UI\n",
        "tickers = get_nifty50_tickers()\n",
        "\n",
        "gr.Interface(\n",
        "    fn=predict_signal,\n",
        "    inputs=[\n",
        "        gr.Dropdown(tickers, label=\"Choose a Stock\"),\n",
        "        gr.Number(label=\"Capital (₹)\", value=100000)\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Stock BUY Signal Predictor\",\n",
        "    description=\"Predict BUY/HOLD signal based on Kalman-filtered valuation and XGBoost model.\"\n",
        ").launch(pwa=True)"
      ],
      "metadata": {
        "id": "HKaDPRNHOxZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def predict_signal(ticker, capital):\n",
        "    try:\n",
        "        df = cleaned_data_2.get(ticker)\n",
        "        if df is None or len(df) < 100:\n",
        "            raise ValueError(\"Not enough data to analyze this stock.\")\n",
        "\n",
        "        fe_df, mapping = feature_engineering_simplified(df)\n",
        "        df_with_target = generate_kalman_target(fe_df, mapping, 0.985, 5)\n",
        "\n",
        "        model = model_results[ticker][\"model\"]\n",
        "        features = model_results[ticker][\"feature_names\"]\n",
        "\n",
        "        latest = df_with_target.iloc[[-1]][features]\n",
        "        prob, pred = make_predictions(model, latest, features)\n",
        "\n",
        "        decision = \"🟢 BUY\" if pred[0] == 1 else \"🔴 HOLD\"\n",
        "        probability = f\"{prob[0]*100:.2f}%\"\n",
        "        capital_fmt = f\"₹{int(capital):,}\"\n",
        "\n",
        "        # Plot actual vs fair price\n",
        "        plot_df = df_with_target.copy()\n",
        "        fair_col = [c for c in plot_df.columns if \"kalman\" in c.lower()]\n",
        "        plot_df[\"Fair\"] = plot_df[fair_col[0]] if fair_col else df[\"close\"]\n",
        "        plot_df[\"Close\"] = df[\"close\"]\n",
        "        plot_df = plot_df[-100:]\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(plot_df.index, plot_df[\"Close\"], label=\"Actual Price\", linewidth=2)\n",
        "        plt.plot(plot_df.index, plot_df[\"Fair\"], label=\"Fair Price (Kalman)\", linewidth=2)\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.title(f\"{ticker} - Price vs Fair Value\")\n",
        "        plt.xlabel(\"Date\")\n",
        "        plt.ylabel(\"Price\")\n",
        "\n",
        "        return {\n",
        "            \"Stock\": ticker,\n",
        "            \"Decision\": decision,\n",
        "            \"Probability\": probability,\n",
        "            \"Capital Allocated\": capital_fmt\n",
        "        }, plt\n",
        "\n",
        "    except Exception:\n",
        "        return {\n",
        "            \"Stock\": ticker,\n",
        "            \"Decision\": \"❓ Unknown\",\n",
        "            \"Probability\": \"N/A\",\n",
        "            \"Capital Allocated\": f\"₹{int(capital):,}\",\n",
        "            \"Message\": \"Not enough information to generate prediction.\"\n",
        "        }, None"
      ],
      "metadata": {
        "id": "UbYjheWoRgnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tickers = get_nifty50_tickers()\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"violet\", secondary_hue=\"violet\")) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "        <div style='text-align:center'>\n",
        "            <h1 style='color:black;'>Walford Capitals Trade Bot</h1>\n",
        "            <h4 style='color:gray;'>Powered by a proprietary model</h4>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        ticker_input = gr.Dropdown(tickers, label=\"Select a Stock\")\n",
        "        capital_input = gr.Slider(20000, 9999999, value=100000, label=\"Capital (₹)\", step=10000)\n",
        "\n",
        "    submit_btn = gr.Button(\"Run Prediction\")\n",
        "\n",
        "    with gr.Row():\n",
        "        result_output = gr.JSON(label=\"Prediction Result\")\n",
        "        chart_output = gr.Plot(label=\"Price vs Fair Value\")\n",
        "\n",
        "    submit_btn.click(fn=predict_signal,\n",
        "                     inputs=[ticker_input, capital_input],\n",
        "                     outputs=[result_output, chart_output])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "eb7xF5r3Rgzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tickers = get_nifty50_tickers()\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"violet\", secondary_hue=\"violet\")) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "        <div style=\"text-align:center; font-family: 'Segoe UI', sans-serif;\">\n",
        "            <h1 style=\"color:white; font-size: 36px;\">Walford Capitals Trade Bot</h1>\n",
        "            <h4 style=\"color:gray; font-size: 18px;\">Powered by a proprietary model</h4>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        ticker_input = gr.Dropdown(tickers, label=\"Select a Stock\", scale=1)\n",
        "        capital_input = gr.Slider(20000, 9999999, value=100000, label=\"Capital (₹)\", step=10000, scale=1)\n",
        "\n",
        "    submit_btn = gr.Button(\"Run Prediction\", scale=2)\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        result_output = gr.JSON(label=\"Prediction Result\", scale=1)\n",
        "        chart_output = gr.Plot(label=\"Price vs Fair Value\", scale=1)\n",
        "\n",
        "    submit_btn.click(fn=predict_signal,\n",
        "                     inputs=[ticker_input, capital_input],\n",
        "                     outputs=[result_output, chart_output])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "X9HQjckXSHgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E-DEV4bC49TX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}